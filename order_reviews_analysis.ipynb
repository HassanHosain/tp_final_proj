{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>review_answer_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7bc2406110b926393aa56f80a40eba40</td>\n",
       "      <td>73fc7af87114b39712e6da79b0a377eb</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-18 00:00:00</td>\n",
       "      <td>2018-01-18 21:46:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80e641a11e56f04c1ad469d5645fdfde</td>\n",
       "      <td>a548910a1c6147796b98fdf73dbeba33</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-10 00:00:00</td>\n",
       "      <td>2018-03-11 03:05:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228ce5500dc1d8e020d8d1322874b6f0</td>\n",
       "      <td>f9e4b658b201a9f2ecdecbb34bed034b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-17 00:00:00</td>\n",
       "      <td>2018-02-18 14:36:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "      <td>2017-04-21 00:00:00</td>\n",
       "      <td>2017-04-21 22:02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "      <td>2018-03-01 00:00:00</td>\n",
       "      <td>2018-03-02 10:26:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
       "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
       "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
       "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
       "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "\n",
       "   review_score review_comment_title  \\\n",
       "0             4                  NaN   \n",
       "1             5                  NaN   \n",
       "2             5                  NaN   \n",
       "3             5                  NaN   \n",
       "4             5                  NaN   \n",
       "\n",
       "                              review_comment_message review_creation_date  \\\n",
       "0                                                NaN  2018-01-18 00:00:00   \n",
       "1                                                NaN  2018-03-10 00:00:00   \n",
       "2                                                NaN  2018-02-17 00:00:00   \n",
       "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
       "4  Parabéns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
       "\n",
       "  review_answer_timestamp  \n",
       "0     2018-01-18 21:46:59  \n",
       "1     2018-03-11 03:05:13  \n",
       "2     2018-02-18 14:36:24  \n",
       "3     2017-04-21 22:02:06  \n",
       "4     2018-03-02 10:26:53  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cleaned_orders_reviews_df = pd.read_csv(\"cleaned_orders_reviews\")\n",
    "\n",
    "cleaned_orders_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we are only focussing on review comment message then we are dropping the rest and store it into a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>translated_comment_title</th>\n",
       "      <th>translated_review_comment</th>\n",
       "      <th>review_creation_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I received it well before the stipulated deadl...</td>\n",
       "      <td>2017-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Congratulations lannister stores I loved shopp...</td>\n",
       "      <td>2018-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8670d52e15e00043ae7de4c01cc2fe06</td>\n",
       "      <td>b9bf720beb4ab3728760088589c62129</td>\n",
       "      <td>4</td>\n",
       "      <td>I recommend</td>\n",
       "      <td>efficient device. On the website the brand of ...</td>\n",
       "      <td>2018-05-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4b49719c8a200003f700d3d986ea1a19</td>\n",
       "      <td>9d6f15f95d01e79bd1349cc208361f09</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>But a little slow...for the price, it's good.</td>\n",
       "      <td>2018-02-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3948b09f7c818e2d86c9a546758b2335</td>\n",
       "      <td>e51478e7e277a83743b6f9991dbfa3fb</td>\n",
       "      <td>5</td>\n",
       "      <td>I highly recommend</td>\n",
       "      <td>Reliable seller, ok product and delivery on time.</td>\n",
       "      <td>2018-05-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
       "1  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "2  8670d52e15e00043ae7de4c01cc2fe06  b9bf720beb4ab3728760088589c62129   \n",
       "3  4b49719c8a200003f700d3d986ea1a19  9d6f15f95d01e79bd1349cc208361f09   \n",
       "4  3948b09f7c818e2d86c9a546758b2335  e51478e7e277a83743b6f9991dbfa3fb   \n",
       "\n",
       "   review_score translated_comment_title  \\\n",
       "0             5                      NaN   \n",
       "1             5                      NaN   \n",
       "2             4              I recommend   \n",
       "3             4                      NaN   \n",
       "4             5       I highly recommend   \n",
       "\n",
       "                           translated_review_comment review_creation_date  \n",
       "0  I received it well before the stipulated deadl...           2017-04-21  \n",
       "1  Congratulations lannister stores I loved shopp...           2018-03-01  \n",
       "2  efficient device. On the website the brand of ...           2018-05-22  \n",
       "3      But a little slow...for the price, it's good.           2018-02-16  \n",
       "4  Reliable seller, ok product and delivery on time.           2018-05-23  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the null values that are in review_comment_message and reset index and drop the old index column\n",
    "text_analysis_df = cleaned_orders_reviews_df.dropna(subset=['translated_review_comment']).reset_index(drop=True)\n",
    "\n",
    "#text_analysis_df.head()\n",
    "\n",
    "text_analysis_df = text_analysis_df.loc[:,[\"review_id\",\"order_id\", \"review_score\", \"translated_comment_title\" ,\"translated_review_comment\", \"review_creation_date\"]]\n",
    "\n",
    "text_analysis_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n",
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\T470\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\T470\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to extract keywords from the text\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Removing stopwords removing words that is non-aphabetical as well as words that exist in stop words like i.e the\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Frequency count\n",
    "    word_freq = nltk.FreqDist(tokens)\n",
    "\n",
    "    # Create dictionary\n",
    "    word_dict = {word: idx for idx, (word, _) in enumerate(word_freq.items())}\n",
    "\n",
    "    return word_dict\n",
    "\n",
    "    # # keyword_counter = Counter(filtered_tokens)\n",
    "    # return keyword_counter.most_common(num_keywords)\n",
    "\n",
    "# Apply keyword extraction to the 'Comment' column\n",
    "text_analysis_df['Keywords'] = text_analysis_df['translated_review_comment'].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>translated_comment_title</th>\n",
       "      <th>translated_review_comment</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I received it well before the stipulated deadl...</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'i': 0, 'receiv': 1, 'it': 2, 'well': 3, 'bef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Congratulations lannister stores I loved shopp...</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'congratul': 0, 'lannist': 1, 'store': 2, 'i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8670d52e15e00043ae7de4c01cc2fe06</td>\n",
       "      <td>b9bf720beb4ab3728760088589c62129</td>\n",
       "      <td>4</td>\n",
       "      <td>I recommend</td>\n",
       "      <td>efficient device. On the website the brand of ...</td>\n",
       "      <td>2018-05-22</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'effici': 0, 'devic': 1, '.': 2, 'on': 3, 'th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4b49719c8a200003f700d3d986ea1a19</td>\n",
       "      <td>9d6f15f95d01e79bd1349cc208361f09</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>But a little slow...for the price, it's good.</td>\n",
       "      <td>2018-02-16</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'but': 0, 'a': 1, 'littl': 2, 'slow': 3, '......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3948b09f7c818e2d86c9a546758b2335</td>\n",
       "      <td>e51478e7e277a83743b6f9991dbfa3fb</td>\n",
       "      <td>5</td>\n",
       "      <td>I highly recommend</td>\n",
       "      <td>Reliable seller, ok product and delivery on time.</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'reliabl': 0, 'seller': 1, ',': 2, 'ok': 3, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9314d6f9799f5bfba510cc7bcd468c01</td>\n",
       "      <td>0dacf04c5ad59fd5a0cc1faa07c34e39</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I WOULD LIKE TO KNOW WHAT HAPPENED, I ALWAYS R...</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'i': 0, 'would': 1, 'like': 2, 'to': 3, 'know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>373cbeecea8286a2b66c97b1b157ec46</td>\n",
       "      <td>583174fbe37d3d5f0d6661be3aad1786</td>\n",
       "      <td>1</td>\n",
       "      <td>My product didn't arrive</td>\n",
       "      <td>Terrible</td>\n",
       "      <td>2018-08-15</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'terribl': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d21bbc789670eab777d27372ab9094cc</td>\n",
       "      <td>4fc44d78867142c627497b60a7e0228a</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Store note 10</td>\n",
       "      <td>2018-07-10</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'store': 0, 'note': 1, '10': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0e0190b9db53b689b285d3f3916f8441</td>\n",
       "      <td>79832b7cb59ac6f887088ffd686e1d5e</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thank you for the attention you gave me</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'thank': 0, 'you': 1, 'for': 2, 'the': 3, 'at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fe3db7c069d694bab50cc43463f91608</td>\n",
       "      <td>2ca73e2ff9e3a186ad1e1ffb9b1d9c10</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The purchase was made easily. Delivery was mad...</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'the': 0, 'purchas': 1, 'wa': 2, 'made': 3, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
       "1  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "2  8670d52e15e00043ae7de4c01cc2fe06  b9bf720beb4ab3728760088589c62129   \n",
       "3  4b49719c8a200003f700d3d986ea1a19  9d6f15f95d01e79bd1349cc208361f09   \n",
       "4  3948b09f7c818e2d86c9a546758b2335  e51478e7e277a83743b6f9991dbfa3fb   \n",
       "5  9314d6f9799f5bfba510cc7bcd468c01  0dacf04c5ad59fd5a0cc1faa07c34e39   \n",
       "6  373cbeecea8286a2b66c97b1b157ec46  583174fbe37d3d5f0d6661be3aad1786   \n",
       "7  d21bbc789670eab777d27372ab9094cc  4fc44d78867142c627497b60a7e0228a   \n",
       "8  0e0190b9db53b689b285d3f3916f8441  79832b7cb59ac6f887088ffd686e1d5e   \n",
       "9  fe3db7c069d694bab50cc43463f91608  2ca73e2ff9e3a186ad1e1ffb9b1d9c10   \n",
       "\n",
       "   review_score  translated_comment_title  \\\n",
       "0             5                       NaN   \n",
       "1             5                       NaN   \n",
       "2             4               I recommend   \n",
       "3             4                       NaN   \n",
       "4             5        I highly recommend   \n",
       "5             2                       NaN   \n",
       "6             1  My product didn't arrive   \n",
       "7             5                 Excellent   \n",
       "8             5                       NaN   \n",
       "9             5                       NaN   \n",
       "\n",
       "                           translated_review_comment review_creation_date  \\\n",
       "0  I received it well before the stipulated deadl...           2017-04-21   \n",
       "1  Congratulations lannister stores I loved shopp...           2018-03-01   \n",
       "2  efficient device. On the website the brand of ...           2018-05-22   \n",
       "3      But a little slow...for the price, it's good.           2018-02-16   \n",
       "4  Reliable seller, ok product and delivery on time.           2018-05-23   \n",
       "5  I WOULD LIKE TO KNOW WHAT HAPPENED, I ALWAYS R...           2018-01-18   \n",
       "6                                           Terrible           2018-08-15   \n",
       "7                                      Store note 10           2018-07-10   \n",
       "8            thank you for the attention you gave me           2017-12-01   \n",
       "9  The purchase was made easily. Delivery was mad...           2018-03-23   \n",
       "\n",
       "  Sentiment                                           Keywords  \n",
       "0   Neutral  {'i': 0, 'receiv': 1, 'it': 2, 'well': 3, 'bef...  \n",
       "1  Positive  {'congratul': 0, 'lannist': 1, 'store': 2, 'i'...  \n",
       "2   Neutral  {'effici': 0, 'devic': 1, '.': 2, 'on': 3, 'th...  \n",
       "3  Positive  {'but': 0, 'a': 1, 'littl': 2, 'slow': 3, '......  \n",
       "4  Positive  {'reliabl': 0, 'seller': 1, ',': 2, 'ok': 3, '...  \n",
       "5  Negative  {'i': 0, 'would': 1, 'like': 2, 'to': 3, 'know...  \n",
       "6  Negative                                     {'terribl': 0}  \n",
       "7   Neutral                   {'store': 0, 'note': 1, '10': 2}  \n",
       "8   Neutral  {'thank': 0, 'you': 1, 'for': 2, 'the': 3, 'at...  \n",
       "9  Positive  {'the': 0, 'purchas': 1, 'wa': 2, 'made': 3, '...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Visualize word cloud\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mvisualize_word_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_analysis_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKeywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[92], line 6\u001b[0m, in \u001b[0;36mvisualize_word_cloud\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_word_cloud\u001b[39m(text):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Display the generated image:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\T470\\Documents\\temasekpoly_final_proj\\tp_final_proj\\my_virtualenv\\Lib\\site-packages\\wordcloud\\wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\T470\\Documents\\temasekpoly_final_proj\\tp_final_proj\\my_virtualenv\\Lib\\site-packages\\wordcloud\\wordcloud.py:623\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    607\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(words)\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\T470\\Documents\\temasekpoly_final_proj\\tp_final_proj\\my_virtualenv\\Lib\\site-packages\\wordcloud\\wordcloud.py:585\u001b[0m, in \u001b[0;36mWordCloud.process_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    582\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_word_length \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]+\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m regexp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregexp \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregexp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pattern\n\u001b[1;32m--> 585\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m# remove 's\u001b[39;00m\n\u001b[0;32m    587\u001b[0m words \u001b[38;5;241m=\u001b[39m [word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[0;32m    588\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\re\\__init__.py:216\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39mfindall(string)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'Series'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_word_cloud(text):\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize word cloud\n",
    "visualize_word_cloud(text_analysis_df['Keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to perform topic modeling using LDA\n",
    "def perform_topic_modeling(texts, num_topics=5):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    return lda\n",
    "\n",
    "# Apply topic modeling to the 'Comment' column\n",
    "lda_model = perform_topic_modeling(text_analysis_df['translated_review_comment'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
