{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b7c72f-1611-453f-81ca-052c13127d57",
   "metadata": {},
   "source": [
    "## the 8 tables uploaded successfuly but there is no PK and the datetime datatype is not updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8359d4-5d5e-4708-9603-9ce5158195cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameen\\AppData\\Local\\Temp\\ipykernel_10496\\1551092707.py:26: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files uploaded successfully to PostgreSQL database with modified table names, correct data types, and primary key.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, VARCHAR, TIMESTAMP\n",
    "# import os\n",
    "\n",
    "# # Define the folder path\n",
    "# folder_path = 'clean_data/'\n",
    "\n",
    "# # List all files in the folder\n",
    "# files = os.listdir(folder_path)\n",
    "\n",
    "# # Filter out only CSV files\n",
    "# csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "# # Connect to PostgreSQL database\n",
    "# engine = create_engine('postgresql://postgres:123@localhost:5432/olist_db2')\n",
    "\n",
    "# # Read and upload each CSV file\n",
    "# for file in csv_files:\n",
    "#     # Check if file is empty\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     if os.path.getsize(file_path) == 0:\n",
    "#         print(f\"Skipping empty file: {file}\")\n",
    "#         continue\n",
    "    \n",
    "#     # Read CSV into DataFrame\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Define table name without 'cleaned_'\n",
    "#     table_name = file.split('.')[0].replace('cleaned_', '') + '_tbl'\n",
    "    \n",
    "#     # Define data types for each column\n",
    "#     dtypes = {}\n",
    "#     for col, dtype in df.dtypes.items():\n",
    "#         if dtype == 'object':\n",
    "#             dtypes[col] = VARCHAR(length=255)  # Adjust the length as needed\n",
    "#         elif dtype == 'datetime64[ns]':\n",
    "#             dtypes[col] = TIMESTAMP()\n",
    "    \n",
    "#     # Assign primary key\n",
    "#     primary_key = df.columns[0]\n",
    "    \n",
    "#     # Upload DataFrame to database with specified data types and primary key\n",
    "#     df.to_sql(table_name, engine, if_exists='replace', index=False, dtype=dtypes, index_label=primary_key)\n",
    "\n",
    "# # Close the connection\n",
    "# engine.dispose()\n",
    "\n",
    "# print(\"All CSV files uploaded successfully to PostgreSQL database with modified table names, correct data types, and primary key.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b726a6b-5d34-4b7c-91ff-02a156b38ae1",
   "metadata": {},
   "source": [
    "# olist_db3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3ab95-2407-459b-93da-ebc920cf9a49",
   "metadata": {},
   "source": [
    "### Didnt work as review_id not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ac0ea4-8131-4c3e-ab67-b92ca16faf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameen\\AppData\\Local\\Temp\\ipykernel_10496\\550868871.py:29: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "ename": "IntegrityError",
     "evalue": "(psycopg2.errors.UniqueViolation) could not create unique index \"orders_reviews_tbl_pkey\"\nDETAIL:  Key (review_id)=(d433c252647c51309432ca0b763f969b) is duplicated.\n\n[SQL: ALTER TABLE orders_reviews_tbl ADD PRIMARY KEY (review_id)]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUniqueViolation\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1969\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1969\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[0;32m   1970\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[0;32m   1971\u001b[0m         )\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:922\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 922\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[1;31mUniqueViolation\u001b[0m: could not create unique index \"orders_reviews_tbl_pkey\"\nDETAIL:  Key (review_id)=(d433c252647c51309432ca0b763f969b) is duplicated.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Execute the primary key constraint query\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m---> 57\u001b[0m         conn\u001b[38;5;241m.\u001b[39mexecute(primary_key_constraint)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Close the connection\u001b[39;00m\n\u001b[0;32m     60\u001b[0m engine\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1417\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1418\u001b[0m         distilled_parameters,\n\u001b[0;32m   1419\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1420\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py:517\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[1;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\u001b[38;5;241m.\u001b[39m_execute_clauseelement(\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m, distilled_params, execution_options\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1639\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[1;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[0;32m   1627\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[0;32m   1629\u001b[0m )\n\u001b[0;32m   1631\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[0;32m   1632\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[0;32m   1633\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1637\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[0;32m   1638\u001b[0m )\n\u001b[1;32m-> 1639\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_context(\n\u001b[0;32m   1640\u001b[0m     dialect,\n\u001b[0;32m   1641\u001b[0m     dialect\u001b[38;5;241m.\u001b[39mexecution_ctx_cls\u001b[38;5;241m.\u001b[39m_init_compiled,\n\u001b[0;32m   1642\u001b[0m     compiled_sql,\n\u001b[0;32m   1643\u001b[0m     distilled_parameters,\n\u001b[0;32m   1644\u001b[0m     execution_options,\n\u001b[0;32m   1645\u001b[0m     compiled_sql,\n\u001b[0;32m   1646\u001b[0m     distilled_parameters,\n\u001b[0;32m   1647\u001b[0m     elem,\n\u001b[0;32m   1648\u001b[0m     extracted_params,\n\u001b[0;32m   1649\u001b[0m     cache_hit\u001b[38;5;241m=\u001b[39mcache_hit,\n\u001b[0;32m   1650\u001b[0m )\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1654\u001b[0m         elem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1658\u001b[0m         ret,\n\u001b[0;32m   1659\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1848\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(\n\u001b[0;32m   1844\u001b[0m         dialect,\n\u001b[0;32m   1845\u001b[0m         context,\n\u001b[0;32m   1846\u001b[0m     )\n\u001b[0;32m   1847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_single_context(\n\u001b[0;32m   1849\u001b[0m         dialect, context, statement, parameters\n\u001b[0;32m   1850\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1988\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1985\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception(\n\u001b[0;32m   1989\u001b[0m         e, str_statement, effective_parameters, cursor, context\n\u001b[0;32m   1990\u001b[0m     )\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2344\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2343\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1969\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1967\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1969\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[0;32m   1970\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[0;32m   1971\u001b[0m         )\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1975\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1976\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1980\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1981\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:922\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 922\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[1;31mIntegrityError\u001b[0m: (psycopg2.errors.UniqueViolation) could not create unique index \"orders_reviews_tbl_pkey\"\nDETAIL:  Key (review_id)=(d433c252647c51309432ca0b763f969b) is duplicated.\n\n[SQL: ALTER TABLE orders_reviews_tbl ADD PRIMARY KEY (review_id)]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, VARCHAR, TIMESTAMP, MetaData, text\n",
    "# import os\n",
    "\n",
    "# # Define the folder path\n",
    "# folder_path = 'clean_data/'\n",
    "\n",
    "# # List all files in the folder\n",
    "# files = os.listdir(folder_path)\n",
    "\n",
    "# # Filter out only CSV files\n",
    "# csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "# # Connect to PostgreSQL database\n",
    "# engine = create_engine('postgresql://postgres:123@localhost:5432/olist_db3')\n",
    "\n",
    "# # Initialize metadata\n",
    "# metadata = MetaData()\n",
    "\n",
    "# # Read and upload each CSV file\n",
    "# for file in csv_files:\n",
    "#     # Check if file is empty\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     if os.path.getsize(file_path) == 0:\n",
    "#         print(f\"Skipping empty file: {file}\")\n",
    "#         continue\n",
    "    \n",
    "#     # Read CSV into DataFrame\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Define table name without 'cleaned_'\n",
    "#     table_name = file.split('.')[0].replace('cleaned_', '') + '_tbl'\n",
    "    \n",
    "#     # Define data types for each column\n",
    "#     dtypes = {}\n",
    "#     for col, dtype in df.dtypes.items():\n",
    "#         if dtype == 'object':\n",
    "#             dtypes[col] = VARCHAR(length=255)  # Adjust the length as needed\n",
    "#         elif dtype == 'datetime64[ns]':\n",
    "#             dtypes[col] = TIMESTAMP()\n",
    "    \n",
    "#     # Assign primary key\n",
    "#     primary_key = df.columns[0]\n",
    "    \n",
    "#     # Upload DataFrame to database with specified data types and primary key\n",
    "#     df.to_sql(table_name, engine, if_exists='replace', index=False, dtype=dtypes, index_label=primary_key)\n",
    "\n",
    "#     # Reflect the metadata from the database engine\n",
    "#     metadata.reflect(bind=engine)\n",
    "\n",
    "#     # Define the table and primary key constraint\n",
    "#     table = metadata.tables[table_name]\n",
    "#     primary_key_constraint = text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({primary_key})\")\n",
    "    \n",
    "#     # Execute the primary key constraint query\n",
    "#     with engine.connect() as conn:\n",
    "#         conn.execute(primary_key_constraint)\n",
    "\n",
    "# # Close the connection\n",
    "# engine.dispose()\n",
    "\n",
    "# print(\"All CSV files uploaded successfully to PostgreSQL database with modified table names, correct data types, and primary key.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d49510-26d0-4f1d-bc86-5a0f932a58fb",
   "metadata": {},
   "source": [
    "## didnt work as the datetime and primary key is not uploaded to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a997aade-cf6d-4908-9ef2-29c8fc07b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameen\\AppData\\Local\\Temp\\ipykernel_10496\\3436566070.py:29: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files uploaded successfully to PostgreSQL database with modified table names, correct data types, and primary key.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, VARCHAR, TIMESTAMP, MetaData, text\n",
    "# import os\n",
    "\n",
    "# # Define the folder path\n",
    "# folder_path = 'clean_data/'\n",
    "\n",
    "# # List all files in the folder\n",
    "# files = os.listdir(folder_path)\n",
    "\n",
    "# # Filter out only CSV files\n",
    "# csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "# # Connect to PostgreSQL database\n",
    "# engine = create_engine('postgresql://postgres:123@localhost:5432/olist_db3')\n",
    "\n",
    "# # Initialize metadata\n",
    "# metadata = MetaData()\n",
    "\n",
    "# # Read and upload each CSV file\n",
    "# for file in csv_files:\n",
    "#     # Check if file is empty\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     if os.path.getsize(file_path) == 0:\n",
    "#         print(f\"Skipping empty file: {file}\")\n",
    "#         continue\n",
    "    \n",
    "#     # Read CSV into DataFrame\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Define table name without 'cleaned_'\n",
    "#     table_name = file.split('.')[0].replace('cleaned_', '') + '_tbl'\n",
    "    \n",
    "#     # Define data types for each column\n",
    "#     dtypes = {}\n",
    "#     for col, dtype in df.dtypes.items():\n",
    "#         if dtype == 'object':\n",
    "#             dtypes[col] = VARCHAR(length=255)  # Adjust the length as needed\n",
    "#         elif dtype == 'datetime64[ns]':\n",
    "#             dtypes[col] = TIMESTAMP()\n",
    "    \n",
    "#     # Assign primary key\n",
    "#     primary_key = df.columns[0]\n",
    "    \n",
    "#     # Upload DataFrame to database with specified data types and primary key\n",
    "#     df.to_sql(table_name, engine, if_exists='replace', index=False, dtype=dtypes, index_label=primary_key)\n",
    "\n",
    "#     # Reflect the metadata from the database engine\n",
    "#     metadata.reflect(bind=engine)\n",
    "\n",
    "#     # Define the table\n",
    "#     table = metadata.tables[table_name]\n",
    "    \n",
    "#     # If it's the 'orders_reviews_tbl' table, create composite primary key\n",
    "#     if table_name == 'orders_reviews_tbl':\n",
    "#         # Define primary key as a composite key\n",
    "#         primary_key = ['review_id', 'order_id']\n",
    "\n",
    "#         # Create primary key constraint for composite key\n",
    "#         primary_key_constraint = text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({', '.join(primary_key)})\")\n",
    "\n",
    "#         # Execute the primary key constraint query\n",
    "#         with engine.connect() as conn:\n",
    "#             conn.execute(primary_key_constraint)\n",
    "\n",
    "# # Close the connection\n",
    "# engine.dispose()\n",
    "\n",
    "# print(\"All CSV files uploaded successfully to PostgreSQL database with modified table names, correct data types, and primary key.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c3770-2409-440f-9467-c878684f1e8c",
   "metadata": {},
   "source": [
    "# olist_db4-- still no success-same no datetime dtype and no pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed72ce82-1f89-47ff-8ecd-5017f544ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameen\\AppData\\Local\\Temp\\ipykernel_10496\\3920362496.py:29: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files uploaded successfully to PostgreSQL database with modified table names, correct data types, and primary key.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, VARCHAR, TIMESTAMP, MetaData, text\n",
    "# import os\n",
    "\n",
    "# # Define the folder path\n",
    "# folder_path = 'clean_data/'\n",
    "\n",
    "# # List all files in the folder\n",
    "# files = os.listdir(folder_path)\n",
    "\n",
    "# # Filter out only CSV files\n",
    "# csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "# # Connect to PostgreSQL database\n",
    "# engine = create_engine('postgresql://postgres:123@localhost:5432/olist_db4')\n",
    "\n",
    "# # Initialize metadata\n",
    "# metadata = MetaData()\n",
    "\n",
    "# # Read and upload each CSV file\n",
    "# for file in csv_files:\n",
    "#     # Check if file is empty\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     if os.path.getsize(file_path) == 0:\n",
    "#         print(f\"Skipping empty file: {file}\")\n",
    "#         continue\n",
    "    \n",
    "#     # Read CSV into DataFrame\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Define table name without 'cleaned_'\n",
    "#     table_name = file.split('.')[0].replace('cleaned_', '') + '_tbl'\n",
    "    \n",
    "#     # Define data types for each column\n",
    "#     dtypes = {}\n",
    "#     for col, dtype in df.dtypes.items():\n",
    "#         if dtype == 'object':\n",
    "#             dtypes[col] = VARCHAR(length=255)  # Adjust the length as needed\n",
    "#         elif dtype == 'datetime64[ns]':\n",
    "#             dtypes[col] = TIMESTAMP()\n",
    "    \n",
    "#     # Assign primary key\n",
    "#     primary_key = df.columns[0]\n",
    "    \n",
    "#     # Upload DataFrame to database with specified data types\n",
    "#     df.to_sql(table_name, engine, if_exists='replace', index=False, dtype=dtypes, index_label=None)\n",
    "\n",
    "#     # Reflect the metadata from the database engine\n",
    "#     metadata.reflect(bind=engine)\n",
    "\n",
    "#     # Define the table\n",
    "#     table = metadata.tables[table_name]\n",
    "    \n",
    "#     # If it's the 'orders_reviews_tbl' table, create composite primary key\n",
    "#     if table_name == 'orders_reviews_tbl':\n",
    "#         # Define primary key as a composite key\n",
    "#         primary_key = ['review_id', 'order_id']\n",
    "\n",
    "#         # Create primary key constraint for composite key\n",
    "#         primary_key_constraint = text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({', '.join(primary_key)})\")\n",
    "\n",
    "#         # Execute the primary key constraint query\n",
    "#         with engine.connect() as conn:\n",
    "#             conn.execute(primary_key_constraint)\n",
    "#     else:\n",
    "#         # For other tables, add primary key constraint\n",
    "#         primary_key_constraint = text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({primary_key})\")\n",
    "\n",
    "#         # Execute the primary key constraint query\n",
    "#         with engine.connect() as conn:\n",
    "#             conn.execute(primary_key_constraint)\n",
    "\n",
    "# # Close the connection\n",
    "# engine.dispose()\n",
    "\n",
    "# print(\"All CSV files uploaded successfully to PostgreSQL database with modified table names, correct data types, and primary key.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86540376-0171-44f2-8ef7-9cacacf5203d",
   "metadata": {},
   "source": [
    "# olist_db5 - No success -still the date time not changed and no pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c75624a6-16bc-426c-a6a9-f54657e0a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameen\\AppData\\Local\\Temp\\ipykernel_10496\\1829988349.py:32: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files uploaded successfully to PostgreSQL database with modified table names and data types.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, VARCHAR, TIMESTAMP, MetaData, text\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'clean_data/'\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter out only CSV files\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "engine = create_engine('postgresql://postgres:123@localhost:5432/olist_db5')\n",
    "\n",
    "# Initialize metadata without binding to the engine\n",
    "metadata = MetaData()\n",
    "\n",
    "# Reflect all tables in the database\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Read and upload each CSV file\n",
    "for file in csv_files:\n",
    "    # Check if file is empty\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        print(f\"Skipping empty file: {file}\")\n",
    "        continue\n",
    "    \n",
    "    # Read CSV into DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define table name without 'cleaned_'\n",
    "    table_name = file.split('.')[0].replace('cleaned_', '') + '_tbl'\n",
    "    \n",
    "    # Upload DataFrame to database\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "    # Reflect the newly created table\n",
    "    table = metadata.tables.get(table_name)\n",
    "\n",
    "    # Check if the table is 'orders_reviews_tbl'\n",
    "    if table_name == 'orders_reviews_tbl':\n",
    "        # Define composite primary key\n",
    "        primary_key = 'review_id, order_id'\n",
    "\n",
    "        # Create primary key constraint for composite key\n",
    "        primary_key_constraint = text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({primary_key})\")\n",
    "\n",
    "        # Execute the primary key constraint query\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(primary_key_constraint)\n",
    "\n",
    "    # Check if the table has a datetime column\n",
    "    if any(df.dtypes == 'datetime64[ns]'):\n",
    "        # Loop through columns to change datetime to timestamp\n",
    "        for column in table.columns:\n",
    "            if str(column.type) == 'DATETIME':\n",
    "                column.type = TIMESTAMP()\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()\n",
    "\n",
    "print(\"All CSV files uploaded successfully to PostgreSQL database with modified table names and data types.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571cd97-18b5-4508-9632-e9042dcca7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
